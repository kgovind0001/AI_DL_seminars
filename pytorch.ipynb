{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af5ad82",
   "metadata": {},
   "source": [
    "Introduction to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f932cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy array \n",
    "data = [[1, 2],[3, 4]]\n",
    "np_data = np.array(data)\n",
    "\n",
    "print(np_data.shape , type(np_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c86f40",
   "metadata": {},
   "source": [
    "Tensors are similar to NumPyâ€™s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752e28d",
   "metadata": {},
   "source": [
    "# 3 ways to create a tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eedc28c",
   "metadata": {},
   "source": [
    "Directly from tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79476c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "print(x_data , x_data.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1fcea2",
   "metadata": {},
   "source": [
    "from another numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor(np_data)\n",
    "print(x_data , x_data.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954d0f9",
   "metadata": {},
   "source": [
    "from another tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75320610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some attributes of tensor\n",
    "\n",
    "print(f\"Shape of tensor: {x_rand.shape}\")\n",
    "print(f\"Datatype of tensor: {x_rand.dtype}\")\n",
    "print(f\"Device tensor is stored on: {x_rand.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34b989",
   "metadata": {},
   "source": [
    "## Tensor operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467fef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rand*x_rand # elementwise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89922ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rand.sin() # sine of each element of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rand.add(x_rand).view(1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80146814",
   "metadata": {},
   "source": [
    "# Why use tensor at all \n",
    "\n",
    "1. We can use GPU for computations (x50)\n",
    "2. Allows us to generate a computional graph and create gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rand = x_rand.to(\"cuda\") # moving tensor to gpu \n",
    "print(f\"Device tensor is stored on: {x_rand.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"gradient calculation wrt to tensor: {x_rand.requires_grad}\")\n",
    "\n",
    "x_rand[1,1] = 100 # \n",
    "x_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002871b",
   "metadata": {},
   "source": [
    "#### Autograd \n",
    "\n",
    "source: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "print(f\"output is {Q}\")\n",
    "\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad , b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70f005",
   "metadata": {},
   "source": [
    "##  We have 3 components to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "# input \n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-np.pi, np.pi, 2000, device=device, dtype=dtype)\n",
    "print(x.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ac2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output \n",
    "\n",
    "y = x.sin() # sin of each element\n",
    "\n",
    "plt.plot(x,y,\"ro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266473aa",
   "metadata": {},
   "source": [
    "objective is to define a model to represent such data. In most cases we dont know the output functions. The model should be able to give us the output given an input. \n",
    "\n",
    "##### How do we know if it is the right model \n",
    "To measure the quality of model, we have to define a value such that it is lowest when we get to the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of models \n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec30b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-np.pi, np.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(1):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0923c7",
   "metadata": {},
   "source": [
    "Sources \n",
    "\n",
    "https://www.javatpoint.com/pytorch-vs-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a17ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
